#!/usr/bin/env python3
"""Extract structured data from parsed claim documents using Claude.

Reads all .md files referenced in sources_index.json, sends them to Claude
with the target schema, and produces data.json with citation bboxes.

Usage:
    python3 scripts/extract_schema.py

Requires:
    - ANTHROPIC_API_KEY environment variable
    - sources_index.json (generated by batch_parse.py)
    - .reducto.json files (generated by batch_parse.py)
    - .md files (generated by batch_parse.py)
"""

from __future__ import annotations

import json
import os
import sys
from pathlib import Path

try:
    import anthropic
except ImportError:
    print("Error: anthropic package not installed. Run: pip install anthropic")
    sys.exit(1)

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_DIR = SCRIPT_DIR.parent
DATA_DIR = PROJECT_DIR / "public" / "data"
SCHEMA_PATH = SCRIPT_DIR / "schema.json"

# Import citation finders for direct use (avoids subprocess overhead)
sys.path.insert(0, str(SCRIPT_DIR))
from find_citation import find_citation, find_md_citation


def get_field_type(prop: dict) -> str:
    """Determine the effective type of a schema property."""
    t = prop.get("type", "string")
    if isinstance(t, list):
        # e.g. ["string", "null"] -> "string", ["boolean", "null"] -> "boolean"
        for item in t:
            if item != "null":
                return item
        return "string"
    return t


def flatten_schema(schema: dict, prefix: str = "") -> list[dict]:
    """Flatten nested schema into list of {key, description, query, type, enum, is_array, array_item_fields} dicts."""
    fields = []
    props = schema.get("properties", {})
    for key, prop in props.items():
        full_key = f"{prefix}{key}" if prefix else key
        field_type = get_field_type(prop)

        # Nested object with sub-properties — recurse
        if field_type == "object" and "properties" in prop:
            fields.extend(flatten_schema(prop, prefix=f"{full_key}."))
            continue

        # Array of objects — special handling
        if field_type == "array" and "items" in prop:
            items = prop["items"]
            item_fields = []
            if items.get("type") == "object" and "properties" in items:
                for item_key, item_prop in items["properties"].items():
                    item_fields.append({
                        "key": item_key,
                        "description": item_prop.get("description", ""),
                        "type": get_field_type(item_prop),
                    })
            field = {
                "key": full_key,
                "description": prop.get("description", ""),
                "type": "array",
                "is_array": True,
                "array_item_fields": item_fields,
            }
            query = prop.get("x_extraction", {}).get("query")
            if query:
                field["query"] = query
            fields.append(field)
            continue

        # Leaf field (string, boolean, number, etc.)
        field = {
            "key": full_key,
            "description": prop.get("description", ""),
            "type": field_type,
        }
        if prop.get("enum"):
            field["enum"] = [v for v in prop["enum"] if v is not None]
        query = prop.get("x_extraction", {}).get("query")
        if query:
            field["query"] = query
        fields.append(field)

    return fields


def build_extraction_prompt(schema_fields: list[dict], sources: list[dict], documents: dict[str, str]) -> str:
    """Build the prompt for Claude to extract data from documents."""
    fields_desc = []
    for f in schema_fields:
        type_hint = f["type"]
        if type_hint == "boolean":
            type_hint = "boolean (true/false)"
        if f.get("enum"):
            type_hint = f"one of: {', '.join(str(v) for v in f['enum'])}"

        line = f"- **{f['key']}** ({type_hint}): {f['description']}"
        if f.get("query"):
            queries = f["query"] if isinstance(f["query"], list) else [f["query"]]
            line += f" (look for: {'; '.join(q for q in queries if q)})"

        if f.get("is_array"):
            item_desc = ", ".join(
                f"{sf['key']} ({sf['type']}): {sf['description']}"
                for sf in f.get("array_item_fields", [])
            )
            line += f"\n  Array items have fields: [{item_desc}]"

        fields_desc.append(line)

    docs_section = []
    for src in sources:
        sid = src["id"]
        md = documents.get(sid, "")
        if md:
            docs_section.append(f"### Source: {sid}\nTitle: {src['name']}\n\n{md}")

    num_fields = len(schema_fields)

    return f"""You are an expert data extraction agent for workers' compensation insurance claims.

You have a set of documents from a single claim file. Your task is to extract specific fields from these documents.

## Target Fields

{chr(10).join(fields_desc)}

## Documents

{chr(10).join(docs_section)}

## Instructions

1. Read through ALL documents carefully.
2. For each target field, find the most authoritative value across all documents.
3. For each value you extract, record the EXACT text snippet from the document that contains or supports the value.
   - The snippet must be a consecutive run of plain text as it appears in the document (15-80 characters ideal).
   - Do NOT include HTML tags in snippets — extract the plain text content only.
4. A single field may have citations from multiple documents if the information appears in several places. Include up to 3 citations per field.
5. If a field cannot be determined from the available documents, set its value to null.
6. For boolean fields, use true or false (not strings).
7. For array fields (like filed_liens, prior_claims), return an array of objects. Each array item should have its own citations.
8. For enum fields, use one of the allowed values exactly.

## Output Format

Return a JSON object with this exact structure:

```json
{{
  "extractions": [
    {{
      "field_key": "claimed_injury_description.date_of_injury",
      "value": "09/12/2024",
      "citations": [
        {{
          "source_id": "adjuster-activity-log-claim-notes",
          "text_snippet": "date of injury 09/12/2024 at about 19:15"
        }}
      ]
    }},
    {{
      "field_key": "litigation.is_litigated",
      "value": true,
      "citations": [
        {{
          "source_id": "adjuster-activity-log-claim-notes",
          "text_snippet": "Claim now litigated"
        }}
      ]
    }},
    {{
      "field_key": "existing_claims.prior_claims",
      "value": [
        {{
          "claim_number": "18001234",
          "date_of_injury": "03/15/2018",
          "body_parts": "lumbar spine",
          "status": "Closed",
          "pd_rating": null,
          "settlement_info": null
        }}
      ],
      "citations": [
        {{
          "source_id": "adjuster-activity-log-claim-notes",
          "text_snippet": "prior lumbar claim in 2018 acknowledged"
        }}
      ]
    }},
    ...
  ]
}}
```

Important rules:
- text_snippet MUST be an exact substring from the source document — do not paraphrase
- Do NOT include HTML tags (<tr>, <td>, <th>, <br/>, etc.) in text_snippet — use plain text only
- Keep snippets concise (15-80 chars) — just enough to locate the value in the document
- Include ALL {num_fields} fields in your output, even if value is null (with empty citations)
- Return ONLY the JSON object, no other text"""


def call_claude(prompt: str) -> str:
    """Call Claude API and return the response text."""
    client = anthropic.Anthropic()  # Uses ANTHROPIC_API_KEY env var

    print("Calling Claude for extraction...")
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=16384,
        messages=[{"role": "user", "content": prompt}],
    )

    return response.content[0].text


def parse_extraction_response(response_text: str) -> list[dict]:
    """Parse Claude's JSON response into extraction records."""
    text = response_text.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        text = "\n".join(lines[1:-1])

    data = json.loads(text)
    return data.get("extractions", [])


def resolve_citations(extractions: list[dict], sources_index: list[dict] | None = None) -> list[dict]:
    """Resolve text snippets to citations using find_citation or find_md_citation.

    If sources_index is provided, routes to the appropriate citation finder
    based on source type. Otherwise falls back to find_citation (PDF) for all.
    """
    # Build source type lookup
    source_types: dict[str, str] = {}
    if sources_index:
        for src in sources_index:
            source_types[src["id"]] = src.get("type", "pdf")

    resolved = []
    total = sum(len(e.get("citations", [])) for e in extractions)
    done = 0

    for ext in extractions:
        field_citations = []
        for cit in ext.get("citations", []):
            done += 1
            source_id = cit.get("source_id", "")
            snippet = cit.get("text_snippet", "")
            if not source_id or not snippet:
                continue

            src_type = source_types.get(source_id, "pdf")

            if src_type == "md":
                result = find_md_citation(source_id, snippet)
            else:
                result = find_citation(source_id, snippet)

            if result and "error" not in result:
                field_citations.append(result)
                status = f"ok ({result.get('type', 'pdf')})"
            else:
                status = "no match"
            print(f"  [{done}/{total}] {ext['field_key']}: {source_id} [{src_type}] -> {status}")

        resolved.append({
            "field_key": ext["field_key"],
            "value": ext.get("value"),
            "citations": field_citations,
        })

    return resolved


def get_category(field_key: str) -> str:
    """Extract the top-level category from a dotted field key."""
    return field_key.split(".")[0]


# Map top-level schema key -> display name for category headers
CATEGORY_LABELS = {
    "claimed_injury_description": "Claimed Injury Description",
    "compensability": "Compensability",
    "coverage_employment": "Coverage & Employment",
    "employment_status": "Employment Status",
    "accepted_body_parts": "Accepted Body Parts",
    "non_admitted_body_parts": "Non-Admitted Body Parts",
    "medical_status": "Medical Status",
    "mmi_status": "MMI Status",
    "current_work_status": "Current Work Status",
    "temporary_disability": "Temporary Disability",
    "permanent_disability": "Permanent Disability",
    "permanent_work_restrictions": "Permanent Work Restrictions",
    "apportionment": "Apportionment",
    "vocational_rehab_sjdb": "Vocational Rehab / SJDB",
    "litigation": "Litigation",
    "settlement": "Settlement",
    "medicare_msa": "Medicare / MSA",
    "liens": "Liens",
    "reserves": "Reserves",
    "excess_carrier": "Excess Carrier",
    "buffer_layer_carrier": "Buffer Layer Carrier",
    "subrogation_recovery": "Subrogation / Recovery",
    "investigation_notes": "Investigation Notes",
    "iso_eams_notes": "ISO / EAMS Notes",
    "existing_claims": "Existing Claims",
    "contribution_reimbursement": "Contribution / Reimbursement",
    "plan_of_action": "Plan of Action",
    "metadata": "Metadata",
}


def make_label(field_key: str) -> str:
    """Convert a dotted field key to a human-readable label."""
    parts = field_key.split(".")
    last = parts[-1]
    return last.replace("_", " ").title()


def format_array_value(items: list) -> str:
    """Format an array of objects into a readable string."""
    if not items:
        return "None"
    parts = []
    for i, item in enumerate(items, 1):
        if isinstance(item, dict):
            fields = []
            for k, v in item.items():
                if v is not None:
                    label = k.replace("_", " ").title()
                    fields.append(f"{label}: {v}")
            parts.append(f"({i}) " + "; ".join(fields))
        else:
            parts.append(f"({i}) {item}")
    return " | ".join(parts)


def assemble_data_json(sources: list[dict], resolved: list[dict],
                       schema: dict | None = None) -> dict:
    """Assemble the final data.json structure for the viewer."""
    viewer_sources = []
    for src in sources:
        entry = {
            "id": src["id"],
            "name": src["name"],
            "file": src["file"],
            "pageCount": src["pageCount"],
        }
        # Include type for non-PDF sources
        src_type = src.get("type", "pdf")
        if src_type != "pdf":
            entry["type"] = src_type
        # Include mdFile for markdown sources so the viewer can fetch it
        if src.get("mdFile"):
            entry["mdFile"] = src["mdFile"]
        viewer_sources.append(entry)

    fields = []
    field_counter = 0
    for ext in resolved:
        value = ext["value"]
        if value is None:
            continue

        field_counter += 1
        category = get_category(ext["field_key"])

        # Format display value based on type
        if isinstance(value, bool):
            display_value = "Yes" if value else "No"
        elif isinstance(value, list):
            display_value = format_array_value(value)
        elif isinstance(value, (int, float)):
            display_value = str(value)
        else:
            display_value = str(value)

        fields.append({
            "id": f"f{field_counter}",
            "label": make_label(ext["field_key"]),
            "value": display_value,
            "citations": ext["citations"],
            "category": CATEGORY_LABELS.get(category, category.replace("_", " ").title()),
        })

    return {
        "sources": viewer_sources,
        "fields": fields,
    }


def main():
    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        print("Error: ANTHROPIC_API_KEY environment variable not set.")
        sys.exit(1)

    # Load sources index
    index_path = DATA_DIR / "sources_index.json"
    if not index_path.exists():
        print(f"Error: {index_path} not found. Run batch_parse.py first.")
        sys.exit(1)

    with open(index_path) as f:
        sources = json.load(f)
    print(f"Loaded {len(sources)} sources from index")

    # Load schema
    with open(SCHEMA_PATH) as f:
        schema = json.load(f)
    schema_fields = flatten_schema(schema)
    print(f"Schema has {len(schema_fields)} fields to extract")

    # Load all markdown documents
    documents = {}
    for src in sources:
        md_path = DATA_DIR / f"{src['id']}.md"
        if md_path.exists():
            with open(md_path) as f:
                documents[src["id"]] = f.read()
        else:
            print(f"  Warning: {md_path.name} not found, skipping")
    print(f"Loaded {len(documents)} markdown documents\n")

    # Build prompt and call Claude
    prompt = build_extraction_prompt(schema_fields, sources, documents)
    print(f"Prompt size: ~{len(prompt)} chars")
    response_text = call_claude(prompt)

    # Save raw extraction output for debugging
    raw_output_path = DATA_DIR / "extraction_raw.json"
    with open(raw_output_path, "w") as f:
        f.write(response_text)
    print(f"Saved raw extraction to {raw_output_path.name}")

    # Parse response
    extractions = parse_extraction_response(response_text)
    print(f"Extracted {len(extractions)} fields\n")

    # Resolve citations (routes to PDF bbox or MD citation based on source type)
    print("Resolving citations...")
    resolved = resolve_citations(extractions, sources_index=sources)

    # Count successful citations
    total_citations = sum(len(r["citations"]) for r in resolved)
    fields_with_citations = sum(1 for r in resolved if r["citations"])
    print(f"\nResolved {total_citations} citations across {fields_with_citations} fields\n")

    # Assemble and write data.json
    data = assemble_data_json(sources, resolved, schema)
    output_path = DATA_DIR / "data.json"
    with open(output_path, "w") as f:
        json.dump(data, f, indent=2)
    print(f"Wrote {output_path} with {len(data['sources'])} sources and {len(data['fields'])} fields")


if __name__ == "__main__":
    main()
